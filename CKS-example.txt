1. Kube-bench repairs unsafe items:
When running the CIS benchmarking tool against a kubeadm-created cluster, several issues
were discovered that must be addressed immediately.
task
Fix any issues with configuration and restart the affected components to ensure the new
settings take effect.
Fix all of the following violations found against the API server:
1.2.7 Ensure that the --authorization-mode argument is not set to AlwaysAllow FAIL
1.2.8 Ensure that the --authorization-mode argument includes Node FAIL
1.2.9 Ensure that the --authorization-mode argument includes RBAC FAIL
1.2.18 Ensure that the --insecure-bind-address argument is not set FAIL (This item is not
given in the v1.26 exam questions, but it is best to check it, the simulation environment is
required 
Fix all of the following violations found against the kubelet:
Fix all of the following violations that were found against the kubelet:
4.2.1 Ensure that the anonymous-auth argument is set to false FAIL
4.2.2 Ensure that the --authorization-mode argument is not set to AlwaysAllow FAIL
Note: Use webhook authentication/authorization whenever possible.
Fix all of the following violations found against etcd:
Fix all of the following violations that were found against etcd:
2.2 Ensure that the --client-cert-auth argument is set to true FAIL
In the simulation environment, the script to initialize this question is kube-bench.sh 

- Nhớ các đường dẫn của etcd, kube-apiserver, kubelet
- Fix các issues theo guide của CIS


----------------------------------------
2. Pod specifies ServiceAccount
Your organization's security policy includes:
⚫ ServiceAccount must not automatically mount API credentials
⚫ ServiceAccount name must end with "-sa"
The Pod specified in the manifest file /cks/sa/pod1.yaml cannot be scheduled due to an incorrect ServiceAccount specification.
Please complete the following items:
task
\1. Create a new ServiceAccount named backend-sa in the existing namespace qa ,
Make sure this ServiceAccount does not automatically mount API credentials.
\2. Use the manifest file in /cks/sa/pod1.yaml to create a Pod.
\3. Finally, clean up any unused ServiceAccounts in namespace qa.
1. Create a new sa yaml file.
2. Modify the pod file in the existing yaml
3. First check sa in grep, then delete.


- Document name: configure service account for pod
    + 1. add automountServiceAccountToken: false vào serviceAccount để nó không tự động mount api credential vào pod
    + 2. Create pod với đường dẫn chỉ định /cks/sa/pod1.yaml (nhớ add sa vào file)
    + 3. Get SA và xóa toàn bộ sa không sử dụng

----------------------------------------
3. Default Network
A default-deny NetworkPolicy avoids accidentally exposing Pods in a namespace that does not define any other NetworkPolicy. 
task
Create a new default deny NetworkPolicy named denypolicy in namespace testing for all traffic of type Ingress + Egress.
This new NetworkPolicy must deny all Ingress + Egress traffic in namespace testing.
Apply the newly created default deny NetworkPolicy to all Pods running in namespace testing.

Noted:
# vi denypolicy.yaml

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: denypolicy
  namespace: testing
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress


----------------------------------------
4. RBAC-RoleBinding
Context
The Role bound to the Pod's ServiceAccount grants overly permissive permissions.
Complete the following items to reduce the permission set.
task
An existing Pod named web-pod is already running in namespace db.
Edit the existing Role bound to the Pod's ServiceAccount service-account-web to only allow get operations on resources of type services only.
Create a new role named role-2 in the namespace db that only allows delete operations on resources of type namespaces.
Create a new RoleBinding called role-2-binding to bind the newly created Role to the Pod's ServiceAccount.
Note: Do not delete existing RoleBindings. 

#noted:
Step 1:
    Check name sa: #kubectl get pod web-pod -n db -oyaml
    Sau đó check rolebinding mount sa đó để tìm role: #k describe roleinding -n db role-1-binding
    Tiếp theo edit ver(permission) of role-1

Step 2:
    Create new role: #k create role-2 --verb=delete --resource=namespaces -n db
Step 3:
    Create new rolebinding: #k create rolebinding --role=role-2 --serviceacount=db:service-account-web -n db



----------------------------------------
5. Log audit
Enables audit logging in the cluster. To do this, enable the logging backend, and make sure:
⚫ Logs are stored in /var/log/kubernetes/audit-logs.txt
⚫ Log files can be kept for 10 days
⚫ Keep up to 2 old audit log files
/etc/kubernetes/logpolicy/sample-policy.yaml provides the basic policy. It only specifies what not to log.
Note: The basic policy is located on the master node of the cluster.
Edit and extend the base policy to document:
⚫ Persistentvolumes change at RequestResponse level
⚫ Request body for configmaps changes in namespace front-apps
⚫ Changes to ConfigMap and Secret in all namespaces at Metadata level
Also, add a catch-all rule to log all other requests at the Metadata level.
NOTE: Don't forget to apply the modified policy.

#note:
search: auditing
Step 1:
    - Đầu tiên là overview cái file /etc/kubernetes/logpolicy/sample-policy.yaml
    - Tiếp theo vào guide của auditing và edit base polocy theo yêu cầu của bài (Đọc kĩ các text node trong file auditing của guide nó trùng với đề bài)
Step 2:
    - Tiếp theo kiểm tra thư mục log đề bài đưa ra xem đã tổn tại folder chưa. Nếu chưa thì tạo folder đấy (/var/log/kubernetes)
    - Edit /etc/kubernetes/manifest/kube-apiserver.yaml
        + add file policy: - --audit-policy-file=/etc/kubernetes/audit-policy.yaml
        + add file logs: - --audit-log-path=/var/log/kubernetes/audit/audit.log
        + add log file giữ trong bao nhiêu này: --audit-log-maxage=10
        + add log file sẽ được backup bao nhiêu file logs: --audit-log-maxbackup=2
            => Các cái này đều có trong guide
        
        + add volume: copy và edit theo guide luôn. Lưu ý là chỉ mount foler log chứ k mount file
    - Sau khi làm xong các bước trên thì restart kubelet đợi kube-apiserver restart thành công thì check file output log trong /var/log/kubernetes


----------------------------------------
6. Create secret
task
Get the contents of an existing secret named db1-test in namespace istio-system
Store the username field in a file named /cks/sec/user.txt and the password field in a file
named /cks/sec/pass.txt.
NOTE: You must create the above two files, they don't exist yet.
NOTE: Do not use/modify previously created files in the following steps, create new
temporary files if needed.
Create a new secret named db2-test in the istio-system namespace with the following
content:
username : production-instance
password : KvLftKgs4aVH
Finally, create a new Pod that can access the secret db2-test via the volume:
Pod name secret-pod
Namespace istio-system
Container name dev-container
Mirror nginx
Volume name secret-volume
Mount path /etc/secret 

=> Câu này thì cứ theo đề mà làm
    + Đầu tiên tạo cái folder /cks/sec
    + Sau đó get value của username trong secret lưu vào /cks/sec/user.txt
    + Sau đó get value của password trong secret lưu vào /cks/sec/pass.txt
    + next, create new secret db2-test
    + Sau đó create pod và mount secret nó vào pod
----------------------------------------
7. Dockerfile detection
task
Analyze and edit a given Dockerfile /cks/docker/Dockerfile (based on ubuntu:16.04 image),
And fix two directives that have outstanding security/best practice issues in the file.
Analyze and edit the given manifest file /cks/docker/deployment.yaml,
And fix two fields that have prominent security/best practice issues in the file.
Note: Do not add or remove configuration settings; just modify existing configuration settings so that neither configuration setting above is a
security/best practice issue.
Note: If you need an unprivileged user to execute any projects, use user nobody with user ID 65535.

Noted:
    + Chỉ modify và không xóa bất cứ nội dung gì
    Step 1:
        + Thay đổi user root trong file thành nobody
        + Thay đổi image thành image theo đề bài (ubuntu:16.04)
    Step 2:
        + kiểm tra securityContext trong file deployment -> Chuyển privileged: false và edit RunAsUser: 65535



copy code
vim /cks/docker/ Dockerfile

Change USER root to USER nobody
USER nobody

Modify the base image to the ubuntu required by the title: 16.04 
FROM ubuntu: 16.04
copy code
modify deployment.yaml
# Annotate securityContext

securityContext:
  {'capabilities': {'add': [ET_BIND_SERVICE], 'drop': ['all']}, ' privileged': False, 'readOnlyRootFilesystem': True, 'runAsUser': 65535}




8. Sanbox running container gVisor
Context
The cluster uses containerd as the CRI runtime. The default runtime handler for containerd is
runc.
containerd is ready to support an additional runtime handler runsc (gVisor).
task
Using the existing runtime handler named runsc, create a RuntimeClass named untrusted.
Update all Pods in the namespace server to run on gVisor.
You can find a list of templates in /cks/gVisor/rc.yaml. 

#noted:
Search: RuntimeClass
    -> Create a RuntimeClass name "unstrusted" with handler: runsc -> Edit lại các deployment trong namespace server chạy runtimeClassName: unstrusted

----------------------------------------
9. Network Policy
Create a NetworkPolicy called pod-restriction to restrict access to Pod products-service running in namespace dev-team.
Only allow the following Pods to connect to Pod products-service
⚫ Pods in namespace qa
⚫ Pods in any namespace with label environment: testing
Note: Make sure to apply the NetworkPolicy.
You can find a template manifest file at /cks/net/po.yaml.

#noted: search networkpolicies
- Kiểm tra các namespace của pod-striction và namespace qa
- Define networkpolices -> Lưu ý nó sẽ cần có 2 from với type ingress bởi vì đề yêu cầu là pod in any namespace (lưu ý là namespaceselector: {} và bên dưới nó là podSelector luôn chứ không phải - podSelector)

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: pod-restriction
  namespace: dev-team
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          env: qa
  - from:
    - namespaceSelector: {}
      podSelector:
        matchLabels:
          environment: testing
  podSelector:
    matchLabels:
      run: pod-restriction
  policyTypes:
  - Ingress


----------------------------------------
10. Trivy scans image security vulnerabilities
task
Use the Trivy open source container scanner to detect critically vulnerable images used by
Pods in namespace kamino.
Find images with High or Critical severity vulnerabilities and delete Pods that use those
images.
Note: Trivy is only installed on the master node of the cluster,
Not available on worker nodes.
You must switch to the master node of the cluster to use Trivy

#note: muốn coi command thì cứ trivy image --help
- Để get nhanh các image trong namespace kamino thì sử dụng command: #kubectl get pod -n kamino -p custom-columns="NAME:.metadata.name,IMAGE:.spec.containers[*].image"
- Sau đấy sử dụng command: trivy image -s HIGH,CRITICAL nginx:1.19   (nói chung là quét từng image cho nhanh)
=> thằng nào nhiều high và critical thì xóa cmn đi. Để lại cái thằng ít nhất

----------------------------------------
11. AppArmor
Context
APPArmor has been enabled on the worker node node02 of the cluster. An APPArmor profile exists, but has not been implemented.
task
On the worker node node02 of the cluster, implement the existing APPArmor configuration file located at /etc/apparmor.d/nginx_apparmor.
Edit the existing manifest file located at /cks/KSSH00401/nginx-deploy.yaml to apply the AppArmor profile.
Finally, the manifest file is applied and the pods specified in it are created.

#noted:
Please note that APPArmor is on the working node in the exam question, so you need to ssh to the working node written at the beginning.
In the simulation environment, you need to ssh to the working node node02.
Start this directory file on node02. Fall back to the original node.
#Add annotations, the name of kubernetes.io/podx should be the same as the name in containers, and nginx-profile-3 is the name of the apparmor policy
module executed on worker node02.
annotations:
container.apparmor.security.beta.kubernetes.io/podx: localhost/nginx-profile-3 #Note to modify.
detection. Verify that the container is actually running with that profile by checking the proc attr for that profile:
kubectl exec podx --cat /proc/1/attr/current


----------------------------------------
12. Sysdig & falco
Task:
Use the runtime detection tool to detect abnormal processes that are generated and executed
frequently in the Pod redis123 single container.
There are two tools available:
⚫ sysdig
⚫ falco 
Note: These tools are only pre-installed on the worker node node02 of the cluster, not on the
master node.
Use tools to analyze for at least 30 seconds, use filters to inspect spawned and executed
processes, write incidents to /opt/KSR00101/incidents/summary file,
It contains detected events in the following format:
timestamp, uid/username, processName

#Answer:

#use falco
crictl ps | grep redis123
falco # /etc/falco/local
- rule: rule1
 desc: rule1
 condition: container.name = "redis123"
 output: "%evt.time,%user.uid,%proc.name"
 priority: WARNING
sudo falco -M 31 -r /etc/falco/falco_rules.local.yaml >> /opt/KSR00101/incidents/summary

#use sysdig
sysdig -M 30 -p "%evt.time,%user.uid,%proc.name" container.id=detected id> /opt/KSR00101/incidents/summary

----------------------------------------
13. Container security context
Context
The Container Security Context should modify the Deployment in a specific namespace.
task
Modify the Deployment secdep in the sec-ns namespace as follows
1. Start the container with a user whose ID is 30000 (set the user ID to: 30000)
2. Do not allow a process to gain privileges beyond its parent process
(allowPrivilegeEscalation is prohibited)
3. Load the root file system of the container in read-only mode (read-only permission to the
root file) 


#Tasks, configure pods and containers, and configure security contexts for pods or containers.
**Lưu ý là sẽ có 2 container nên sẽ phải set cái securityContext với 2 options allowPrivilegeEscalation và readOnlyRootFilesystem cho cả 2 container
securityContext:
  allowPrivilegeEscalation: fasle
  readOnlyRootFilesystem: true

#Còn thằng runAsUser sẽ set bên dưới spec của container ngay dưới template
#spec: Next settings.
securityContext:
 runAsUser: 30000


----------------------------------------
14. Enable API server authentication
Context
The Kubernetes API server for the cluster created by kubeadm, for testing purposes,
Temporary configuration to allow unauthenticated and unauthorized access, granting access
to the anonymous user cluster-admin.
task
Reconfigure the cluster's Kubernetes API server to ensure only authenticated and authorized
REST requests are allowed.
Use authorization mode Node, RBAC and admission controller NodeRestriction.
Remove the ClusterRoleBinding for user system:anonymous to clean up.
Note: All kubectl configuration environments/files are also configured to use
unauthenticated and unauthorized access.
You don't have to change it, but please note that once the cluster is hardened, the kubectl
configuration will not work.
You can use the original kubectl configuration file of the cluster on the master node of the
cluster
/etc/kubernetes/admin.conf to ensure authenticated authorization requests are still allowed. 

#noted
In the simulation environment, the script to initialize this question is api.sh
#Reference, component tools, kube-apiserver
--enable-admission-plugin=NodeRestriction
--authorization-mode=Node,RBAC
kubectl get clusterrolebinding system:anonymous
kubectl delete clusterrolebinding system:anonymous 


----------------------------------------
task
Enhancing kube-apiserver security configuration via TLS requires
1. kube-apiserver can be used except for TLS 1.3 and above, and other versions are not
allowed.
2. Cipher suite is TLS_AES_128_GCM_SHA256
Enhancing ETCD security configuration over TLS requires
1. Cipher suite is TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
#Reference, component, kubeapiserver
ssh master01 sudo -i
--tls-min-version=VersionTLS13
--tls-cipher-suites=TLS_AES_128_GCM_SHA256
etcd.yaml
--cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
kubectl get pod -n kube-system 

----------------------------------------
16. ImagePolicyWebhook container image scanning
A container image scanner is set up on the cluster, but is not yet fully integrated into the
cluster's configuration.
Once complete, the container image scanner should scan and deny use of vulnerable images.
task
Note: You must complete the entire exam on the master node of the cluster, where all
services and files have been prepared and placed.
Given an incomplete configuration in the directory /etc/kubernetes/epconfig,
And a functional container image scanner with HTTPS endpoint https://image-bouncerwebhook.default.svc:1323/image_policy:
1. Enable the necessary plugins to create mirroring policies
2. Validate the control configuration and change it to implicit deny
3. Edit the configuration to correctly point to the provided HTTPS endpoint
Finally, test that the configuration works by trying to deploy the vulnerable resource /cks/img/web1.yaml


https://www.cnblogs.com/dagongzhe/category/2264105.html
